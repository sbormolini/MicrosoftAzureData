{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer data between SQL and spark pool in Azure Synapse Analytics\n",
    "https://docs.microsoft.com/en-us/learn/modules/integrate-sql-apache-spark-pools-azure-synapse-analytics/5-transfer-data-between-sql-spark-pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scala\n",
    "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
    "import org.apache.spark.sql.SqlAnalyticsConnector._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scala\n",
    "val df = spark.read.sqlanalytics(\"<DBName>.<Schema>.<TableName>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scala\n",
    "df.write.sqlanalytics(\"<DBName>.<Schema>.<TableName>\", <TableType>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scala\n",
    "df.write.sqlanalytics(\"<DBName>.<Schema>.<TableName>\", Constants.INTERNAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--For an external table, you need to pre-create the data source and file format in dedicated SQL pool using SQL queries:\n",
    "CREATE EXTERNAL DATA SOURCE <DataSourceName>\n",
    "WITH\n",
    "  ( LOCATION = 'abfss://...' ,\n",
    "    TYPE = HADOOP\n",
    "  ) ;\n",
    "\n",
    "CREATE EXTERNAL FILE FORMAT <FileFormatName>\n",
    "WITH (  \n",
    "    FORMAT_TYPE = PARQUET,  \n",
    "    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'  \n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scala\n",
    "df.write.\n",
    "    option(Constants.DATA_SOURCE, <DataSourceName>).\n",
    "    option(Constants.FILE_FORMAT, <FileFormatName>).\n",
    "    sqlanalytics(\"<DBName>.<Schema>.<TableName>\", Constants.EXTERNAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate between spark and SQL pool in Azure Synapse Analytics\n",
    "https://docs.microsoft.com/en-us/learn/modules/integrate-sql-apache-spark-pools-azure-synapse-analytics/6-authenticate-between-spark-sql-pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scala\n",
    "val df = spark.read.\n",
    "    option(Constants.SERVER, \"samplews.database.windows.net\").\n",
    "    option(Constants.USER, <SQLServer Login UserName>).\n",
    "    option(Constants.PASSWORD, <SQLServer Login Password>).\n",
    "    sqlanalytics(\"<DBName>.<Schema>.<TableName>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scala\n",
    "df.write.\n",
    "    option(Constants.SERVER, \"samplews.database.windows.net\").\n",
    "    option(Constants.USER, <SQLServer Login UserName>).\n",
    "    option(Constants.PASSWORD, <SQLServer Login Password>).\n",
    "    sqlanalytics(\"<DBName>.<Schema>.<TableName>\", <TableType>)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf1c0a11d69423f68cf5365dff1583cdfe3399f26f4533cda5d60e69425a79d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
