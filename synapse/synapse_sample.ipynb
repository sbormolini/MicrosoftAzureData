{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover supported languages in spark notebooks\n",
    "- PySpark (Python)\n",
    "- Spark (Scala)\n",
    "- .NET Spark (C#)\n",
    "- Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "val scalaDataFrame = spark.read.sqlanalytics(\"mySQLPoolDatabase.dbo.mySQLPoolTable\")\n",
    "scalaDataFrame.createOrReplaceTempView( \"mydataframetable\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM mydataframetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "myNewPythonDataFrame = spark.sql(\"SELECT * FROM mydataframetable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a CSV file from an Azure Data Lake Store Gen2 store as an Apache Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "account_name = \"Your account name\"\n",
    "container_name = \"Your container name\"\n",
    "relative_path = \"Your path\"\n",
    "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.%s.dfs.core.windows.net\" %account_name, \"SharedKey\")\n",
    "spark.conf.set(\"fs.azure.account.key.%s.dfs.core.windows.net\" %account_name ,\"Your ADLS Gen2 Primary Key\")\n",
    "\n",
    "df1 = spark.read.option('header', 'true') \\\n",
    "                .option('delimiter', ',') \\\n",
    "                .csv(adls_path + '/Testfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a CSV file from Azure Storage Account as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "blob_account_name = \"Your blob account name\"\n",
    "blob_container_name = \"Your blob container name\"\n",
    "blob_relative_path = \"Your blob relative path\"\n",
    "blob_sas_token = \"Your blob sas token\"\n",
    "\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "            .option(\"delimiter\",\"|\") \\\n",
    "            .schema(schema) \\\n",
    "            .csv(wasbs_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
